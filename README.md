# ğŸ•·ï¸ Web Scraper for Web Reconnaissance

> This is a custom-built, CLI-based web scraping and reconnaissance toolkit written in Python and Bash. It automates the discovery of valid endpoints via fuzzing, downloads accessible pages, and extracts valuable metadata for use in web enumeration > > and OSINT tasks.
---

## ğŸ“ Project Structure
- â”œâ”€â”€ FUZZ_check.py # Fuzzes the base URL with wordlist
- â”œâ”€â”€ html_download.sh # Downloads each valid endpoint as HTML
- â”œâ”€â”€ data_scrap.py # Parses downloaded HTML to extract useful data
- â”œâ”€â”€ main.sh # Main orchestrator script
- â”œâ”€â”€ FUZZ.txt # Wordlist used for fuzzing
- â””â”€â”€ output_dir/ # Stores downloaded HTML pages (auto-created)


---

## âš™ï¸ Features

- âœ… URL fuzzing using a wordlist  
- âœ… Valid endpoint detection (HTTP 200)  
- âœ… HTML downloading via `curl`  
- âœ… Metadata extraction using BeautifulSoup:
  - Page title
  - Meta tags (name, content)
  - Stylesheet links
  - Hidden input fields
  - Anchor tags

---

## ğŸš€ How to Run

```bash
chmod +x main.sh
./main.sh <url>
```
Make sure FUZZ.txt contains your wordlist before running.
```bash
./main.sh http://example.com
```

## ğŸ§  Dependencies
- Python 3.x
- requests
- beautifulsoup4
- Bash shell with curl
- Linux or WSL recommended
  ```bash
  pip install requests beautifulsoup4
  ```
  
## ğŸ‘¨â€ğŸ’» Author
  **Raihan Shamnad â€” Cybersecurity enthusiast and penetration tester-in-training.**

## ğŸ›¡ï¸ Disclaimer
> This tool is intended for educational and authorized testing purposes only. Do not use it on domains you do not have permission to audit
